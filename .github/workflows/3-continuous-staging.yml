
name: 3 - Continuous Staging & Model Promotion

permissions: write-all

on:
  workflow_run:
    workflows: ["2 - Continuous Delivery"]
    types: [completed]
  workflow_dispatch:

env:
  DOCKER_REGISTRY: ghcr.io
  GITHUB_ACTOR: ${{ github.actor }}
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  COMMIT_SHA: ${{ github.sha }}
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  MLFLOW_MODEL_NAME: ${{ secrets.MODEL_NAME }}
  MLFLOW_EXPERIMENT_NAME: ${{ secrets.EXPERIMENT_NAME }}

jobs:
  stage-tests:
    # Run only if previous stage finished successfully
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: self-hosted

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.13' }
      - run: pip install uv

      - name: Set lowercase image name
        id: image-name
        run: echo "IMAGE_NAME=$(echo ${{ github.repository }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      # Log in to GitHub Container Registry
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - run: docker pull ${{ env.DOCKER_REGISTRY }}/${{ steps.image-name.outputs.IMAGE_NAME }}-inference-api:latest

      - name: Start inference service with staging model
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MLFLOW_MODEL_NAME: ${{ env.MLFLOW_MODEL_NAME }}
          MODEL_ALIAS: staging
        run: |
          docker stop inference-api || true
          docker rm inference-api || true
          docker run -d \
            --name inference-api \
            --network mlops-net \
            -p 9001:9001 \
            -e MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI \
            -e MLFLOW_MODEL_ALIAS=$MODEL_ALIAS \
            -e MLFLOW_MODEL_NAME=$MLFLOW_MODEL_NAME \
            ${{ env.DOCKER_REGISTRY }}/${{ steps.image-name.outputs.IMAGE_NAME }}-inference-api:latest
          
          # Wait for service to be ready
          for i in {1..30}; do
            if curl -f http://localhost:9001/health | grep -q "healthy"; then
              echo "Staging service healthy"
              break
            fi
            sleep 2
          done
          # Fail if not healthy after retries
          if ! curl -f http://localhost:9001/health | grep -q "healthy"; then
            echo "Staging service failed to become healthy"
            exit 1
          fi

      - name: Run E2E tests
        run: uv run pytest tests/test_e2e/e2e_inference.py -v

      - name: Promote model to production
        if: success()
        env:
          MLFLOW_TRACKING_URI: ${{ env.MLFLOW_TRACKING_URI }}
          MODEL_NAME: ${{ secrets.MODEL_NAME }}
          FROM_ALIAS: staging
          TO_ALIAS: production
        run: uv run python model-promotion/promote_model.py

      - name: Stop inference service
        if: always()
        run: |
          docker stop inference-api || true
          docker rm inference-api || true
